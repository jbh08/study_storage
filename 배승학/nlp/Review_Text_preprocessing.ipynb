{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.7 64bit [MSC v.1915 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "7.1.1"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.17134 SP0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.7 64bit [MSC v.1915 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>7.1.1</td></tr><tr><td>OS</td><td>Windows 10 10.0.17134 SP0</td></tr><tr><td colspan='2'>Mon Jul 29 21:35:21 2019 ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.7 64bit [MSC v.1915 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 7.1.1 \\\\ \\hline\n",
       "OS & Windows 10 10.0.17134 SP0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Mon Jul 29 21:35:21 2019 ¢¥eCN©öI¡¾©ö C¡ÍA¨ª¨öA} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<version_information.version_information.VersionInformation object at 0x000002299FB58F98>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%version_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29T21:36:17+09:00\n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 7.1.1\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 \n",
      "\n",
      "numpy 1.15.4\n",
      "pandas 0.23.4\n",
      "nltk 3.3\n",
      "konlpy 0.5.1\n",
      "re 2.2.1\n"
     ]
    }
   ],
   "source": [
    "%watermark -d -p numpy,pandas,nltk,konlpy,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "### What is text preprocessing?\n",
    "\n",
    "- 자연어처리에서 텍스트 전처리는 아주 중요하다. 텍스트 전처리는 용도에 맞게 텍스트를 사전에 처리하는 작업이다. \n",
    "- 텍스트에 대해서 제대로 된 전처리를 하지 않으면 자연어 처리 기법들이 제대로 동작하지 않는다. \n",
    "\n",
    "### 1) 토큰화 (Tokenization)\n",
    "\n",
    "### 2) 정제 (Cleaning) and 정규화 (Normalization)\n",
    "\n",
    "### 3) 불용어 (Stopword)\n",
    "\n",
    "### 4) 정규 표현식 (Regular Expression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1) 토큰화 (Tokenization)\n",
    "\n",
    "### 토큰화란?\n",
    "\n",
    "- 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)하는 일을 해야 한다. \n",
    "\n",
    "\n",
    "- 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(Tokenization)이라고 한다. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의한다. \n",
    "\n",
    "- 한국어의 경우 영어와 문법이 다르기 때문에 형태소 토큰화가 필요할 경우가 있다. \n",
    "\n",
    "\n",
    "### 문장 토큰화 (Sentence Tokenization)\n",
    "\n",
    "- 토큰의 기준을 문장(sentence)로 하는 경우, 문장 토큰화라고 한다. \n",
    "\n",
    "### 단어 토큰화 (Word Tokenization)\n",
    "\n",
    "- 토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화라고 한다. \n",
    "\n",
    "### 구두점 토큰화 (Punctuation Tokenization)\n",
    "\n",
    "- 토큰의 기준을 구두점과 단어로 하는 경우, 구두점 토큰화라고 한다. \n",
    "\n",
    "### 한국어 형태소 분석\n",
    "\n",
    "- 한국어의 경우 영어와 달리 공백으로 뜻의 의미가 나뉘어지는 경우가 아니기 때문에 따로 형태소 분석을 해줘야 하는 경우가 있다. \n",
    "\n",
    "#### 형태소란?\n",
    "\n",
    "- 형태소란 의미를 가지는 최소 단위로써 더 이상 의미적ㅇ로 분석이 불가능한 단위\n",
    "\n",
    "#### 형태소 분석의 한계점\n",
    "\n",
    "- 가끔 형태소분석이 힘든 경우가 있다. 예를 들어 신조어, 은어, 오탈자, 띄어쓰기 등의 오류들이 형태소 분석을 힘들게 한다. \n",
    "\n",
    "- ex) 밤이 왔다 <--> 밤이 맛있다.\n",
    "\n",
    "#### KoNLPy\n",
    "- 한국어 자연어처리를 위한 대표적인 파이썬 라이브러리이다. \n",
    "\n",
    "- Twitter, Komoran, Hannaunum, Mecab, Kokoma 등 5개의 클래스가 있다. \n",
    "\n",
    "- 한국어 형태소 tokenizer를 위해 KoNLPy를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Hello world. It's good to see you. Thanks for buying this book. OLLA!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world.', \"It's good to see you.\", 'Thanks for buying this book.', 'OLLA!']\n"
     ]
    }
   ],
   "source": [
    "# sentence 단위로 tokenize 된것을 볼 수 있다. \n",
    "print(sent_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .가 들어간 구문은 어떻게 처리할까?\n",
    "text2 = \"IP is 192.168.152.1. I am currently looking for a job to apply. It ain't that easy. AMIGO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP is 192.168.152.1.', 'I am currently looking for a job to apply.', \"It ain't that easy.\", 'AMIGO']\n"
     ]
    }
   ],
   "source": [
    "# 기본적으로 \".\"단위로 처리하지만, 예외처리도 한다. \n",
    "print(sent_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Thanks', 'for', 'buying', 'this', 'book', '.', 'OLLA', '!']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위로 짜른 것을 볼 수 있다.\n",
    "print(word_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP', 'is', '192.168.152.1', '.', 'I', 'am', 'currently', 'looking', 'for', 'a', 'job', 'to', 'apply', '.', 'It', 'ai', \"n't\", 'that', 'easy', '.', 'AMIGO']\n"
     ]
    }
   ],
   "source": [
    "# .이 들어간 IP도 잘 처리하였다. \n",
    "print(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구두점 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPunctTokenizer는 클래스이다. 그래서 인스턴스화가 필요하다. \n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Thanks', 'for', 'buying', 'this', 'book', '.', 'OLLA', '!']\n"
     ]
    }
   ],
   "source": [
    "# 구두점 토큰화의 경우 모든 구두점을 기준으로 나눈다. \n",
    "print(tokenizer.tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP', 'is', '192', '.', '168', '.', '152', '.', '1', '.', 'I', 'am', 'currently', 'looking', 'for', 'a', 'job', 'to', 'apply', '.', 'It', 'ain', \"'\", 't', 'that', 'easy', '.', 'AMIGO']\n"
     ]
    }
   ],
   "source": [
    "# 심지어 IP주소의 \".\"도 토큰화한 것을 볼 수 있다. \n",
    "print(tokenizer.tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목적에 따라 sent_tokenize, word_tokenize, WordPuncTokenizer를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요 음 사랑한다는 말이에요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요 음 사랑한다는 말이에요']\n",
      "['이', '밤', '그날의', '반딧불을', '당신의', '창', '가까이', '보낼게요', '음', '사랑한다는', '말이에요']\n",
      "['이', '밤', '그날의', '반딧불을', '당신의', '창', '가까이', '보낼게요', '음', '사랑한다는', '말이에요']\n"
     ]
    }
   ],
   "source": [
    "# nltk를 사용하여 한국어를 분석했을 땐 형태소 분석이 불가능하다. \n",
    "print(sent_tokenize(kor_text))\n",
    "print(word_tokenize(kor_text))\n",
    "print(WordPunctTokenizer().tokenize(kor_text))\n",
    "\n",
    "# 그래서 KoNLPy를 사용해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: JPype1>=0.5.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from konlpy) (0.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konlpy\n",
    "- Konlpy에는 총 5가지의 클래스가 있다. 각 클래스를 사용해보자.\n",
    "- Twitter, Komoran, Hannaumnum, Mecab, Kokoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"아버지가 방에 들어가신다. 안녕하세요. 하늘을 나는 자동차\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "han = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[('아버지', 'ncn'), ('가', 'jcc')], [('아버지', 'ncn'), ('가', 'jcs')]], [[('방', 'nbu'), ('에', 'jca')], [('방', 'ncn'), ('에', 'jca')]], [[('들', 'pvg'), ('어', 'ecx'), ('가', 'px'), ('시', 'ep'), ('ㄴ다', 'ef')], [('듣', 'pvg'), ('어', 'ecx'), ('가', 'px'), ('시', 'ep'), ('ㄴ다', 'ef')], [('들어가', 'pvg'), ('시', 'ep'), ('ㄴ다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]], [], [[('안녕', 'ncps'), ('하세', 'ncpa'), ('요', 'ncn')], [('안녕', 'ncps'), ('하', 'xsms'), ('세요', 'ef')], [('안녕', 'ncps'), ('하', 'xsms'), ('세', 'ef'), ('요', 'jxf')]], [[('.', 'sf')], [('.', 'sy')]], [], [[('하늘', 'ncn'), ('을', 'jco')]], [[('나', 'ncn'), ('는', 'jxc')], [('나', 'npp'), ('는', 'jxc')], [('나', 'pvg'), ('는', 'etm')], [('나', 'px'), ('는', 'etm')], [('나', 'pvg'), ('아', 'ecs'), ('는', 'jxc')], [('나', 'pvg'), ('아', 'ef'), ('는', 'etm')], [('나', 'px'), ('아', 'ecs'), ('는', 'jxc')], [('나', 'px'), ('아', 'ef'), ('는', 'etm')], [('날', 'pvg'), ('는', 'etm')]], [[('자동차', 'ncn')], [('자동', 'ncn'), ('차', 'ncn')]]]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석을 하고 각 토큰에 대해 형태소 후보군을 보여준다. \n",
    "print(han.analyze(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들', '어', '가', '시ㄴ다', '.', '안녕', '하', '세', '요', '.', '하늘', '을', '나', '는', '자동차']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 기준으로 토큰화를 한다. \n",
    "print(han.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '방', '안녕', '하늘', '나', '자동차']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 추출한다. \n",
    "print(han.nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'N'), ('가', 'J'), ('방', 'N'), ('에', 'J'), ('들', 'P'), ('어', 'E'), ('가', 'P'), ('시ㄴ다', 'E'), ('.', 'S'), ('안녕', 'N'), ('하', 'X'), ('세', 'E'), ('요', 'J'), ('.', 'S'), ('하늘', 'N'), ('을', 'J'), ('나', 'N'), ('는', 'J'), ('자동차', 'N')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소분석 및 품사 태깅.\n",
    "print(han.pos(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코모란"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '.', '안녕하세요', '.', '하늘', '을', '나', '는', '자동차']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 기준으로 토큰화한다. \n",
    "print(ko.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('시', 'EP'), ('ㄴ다', 'EF'), ('.', 'SF'), ('안녕하세요', 'NNP'), ('.', 'SF'), ('하늘', 'NNG'), ('을', 'JKO'), ('나', 'NP'), ('는', 'JX'), ('자동차', 'NNG')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소분석 및 품사 태깅.\n",
    "print(ko.pos(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트위터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "tw = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가신다', '.', '안녕하세요', '.', '하늘', '을', '나', '는', '자동차']\n"
     ]
    }
   ],
   "source": [
    "print(tw.morphs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'Noun'), ('가', 'Josa'), ('방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb'), ('.', 'Punctuation'), ('안녕하세요', 'Adjective'), ('.', 'Punctuation'), ('하늘', 'Noun'), ('을', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('자동차', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "print(tw.pos(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 꼬꼬마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('시', 'EPH'), ('ㄴ다', 'EFN'), ('.', 'SF'), ('안녕하', 'VA'), ('세요', 'EFN'), ('.', 'SF'), ('하늘', 'NNG'), ('을', 'JKO'), ('날', 'VV'), ('는', 'ETD'), ('자동차', 'NNG')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '.', '안녕하', '세요', '.', '하늘', '을', '날', '는', '자동차']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.morphs(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 정제 (Cleaning) and 정규화 (Normalization)\n",
    "\n",
    "- 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 항상 함께한다. \n",
    "\n",
    "\n",
    "- 정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하고 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 한다. \n",
    "\n",
    "\n",
    "- 사실 완벽한 정제 작업은 어려운 편이라서, 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾기도 한다.\n",
    "\n",
    "\n",
    "### Cleaning이란?\n",
    "\n",
    "- 갖고 있는 corpus로부터 noise를 제거하는 것. \n",
    "\n",
    "\n",
    "- ex) 대,소문자를 통합, 단 회사이름, 사람이름등은 대소문자 통합을 하지 않는다. \n",
    "\n",
    "\n",
    "- ex) 등장 빈도가 적은 단어 삭제\n",
    "\n",
    "\n",
    "- ex) 길이가 짧은 단어 제거\n",
    "\n",
    "### Normalization이란?\n",
    "\n",
    "- 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다. \n",
    "\n",
    "\n",
    "- ex) 규칙에 기반한 표기가 다른 다른 단어들을 통합해준다. uh-huh와 uhhuh는 다른 표기지만 같은 뜻을 가지고 있다. 또, US와 USA도 다르게 쓰지만 같은 뜻을 가진다. 이런 단어들을 통합시켜서 같은 단어로 만들어 준다. \n",
    "\n",
    "\n",
    "- 코퍼스의 단어의 개수를 줄일 수 있는 normalization 방법으로 어간 추출(Stemming)과 표제어 추출(Lemmatization)이 있다. \n",
    "\n",
    "\n",
    "### 표제어 추출(Lemmatization)\n",
    "\n",
    "- 눈으로 봤을 때는 다른 단어들이지만, 하나의 단어로 일반화 시켜 문서 내의 단어 수를 줄임.\n",
    "\n",
    "\n",
    "- '표제어(Lemma)'는 '기본 사전형 단어'정도의 의미를 갖는다. 즉, 표제어 추출은 단어들로부터 표제어를 찾아가는 과정이다. \n",
    "\n",
    "\n",
    "- 예를 들어, am, are, is는 다 다른 단어이지만 be라는 뿌리에서 나왔다. \n",
    "\n",
    "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것. \n",
    "\n",
    "##### 형태소란?\n",
    "\n",
    "- 형태소란 '의미를 가진 가장 작은 단위'를 뜻한다. 그리고 형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문. \n",
    "\n",
    "\n",
    "- 형태소는 두 가지 종류가 있다. \n",
    "    * 어간(Stem)\n",
    "        : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "    * 접사(affix)\n",
    "        : 단어에 추가적인 의미를 주는 부분.\n",
    "\n",
    "\n",
    "- 형태학적인 파싱은 이 두 가지 요소를 분리하는 작업이다. 예를 들어, cats라는 단어에 대한 형태학적인 파싱을 하면, cat(어간) + s(접사)를 분리한다.  \n",
    "\n",
    "### 어간 추출(Stemming)\n",
    "\n",
    "- 어간을 추출하는 작업을 어간 추출(stemming)이라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "- 하나의 단어로 일반화 시키는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"doing\", \"policy\", \"have\", \"going\", \"dies\", \"died\", \"has\", \"have\", \"watched\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doing', 'policy', 'have', 'going', 'dy', 'died', 'ha', 'have', 'watched']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 별로 제대로 된 데이터를 추출하지 못하였다. \n",
    "[lemma.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사정보를 주어 더 정확하게 추출하도록 하자. \n",
    "words = [[\"doing\", \"v\"], [\"policies\", \"n\"], [\"going\", \"v\"],  [\"dies\", \"v\"], [ \"died\", \"v\"], [\"has\", \"v\"], [\"watched\", \"v\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'policy', 'go', 'die', 'die', 'have', 'watch']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemma.lemmatize(word[0], word[1]) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "- 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"formalize\", \"allowance\", \"electrical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['formal', 'allow', 'electr']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemming.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stopword(불용어)\n",
    "\n",
    "\n",
    "\n",
    "### stopword란?\n",
    "- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다. \n",
    "\n",
    "\n",
    "- 큰 의미가 없다라는 것은 문장 내에서는 자주 등장하지만 문장을 분석하는 데 있어서는 큰 도움이 되지 않는 단어들을 말한다. \n",
    "\n",
    "\n",
    "- 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어에서 불용어\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Family is not an important thing. It's everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'\", 's', 'everything', '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'\", 'everything', '.']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s'가 사라진 것을 볼 수 있다. \n",
    "[_ for _ in WordPunctTokenizer().tokenize(example) if _ not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regular Expression\n",
    "\n",
    "- 텍스트 데이터를 전처리하다보면 정규표현식은 아주 유용한 도구로서 사용된다. \n",
    "\n",
    "\n",
    "- 정규표현식은 re와 NLTK를 통해 토큰화가 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "# compile하기.\n",
    "\n",
    "r = re.compile(\"a.c\")\n",
    "print(r.search(\"abcdefgh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(3, 6), match='abc'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# match : match는 첫 문자부터 적용하여 첫문자부터 맞지 않으면 none\n",
    "# search : search는 순서와 상관 없이 제일 먼저 일치하는 것 찾는다.  \n",
    "\n",
    "r = re.compile(\"ab.\")\n",
    "print(r.search(\"sssabc\"))\n",
    "print(r.match(\"ssabc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '복숭아', '메론', '바나나']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "re.split(\" \", \"사과 복숭아 메론 바나나\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010-3311-2212']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# findall\n",
    "re.findall(\"\\d+-\\d+-\\d+\", \"\"\"이름 : 배승학 전화번호 : 010-3311-2212 성별 : 남자\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' John PROF  James STUD  Mac STUD'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub\n",
    "text = \"\"\"100 John PROF 101 James STUD 102 Mac STUD\"\"\"\n",
    "re.sub(\"[^\\D+]\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK로 정규식 사용하여 토큰화하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
